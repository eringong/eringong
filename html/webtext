HOME

Community organizations face countless decisions in strategic planning, resource allocation, and program design. Program evaluation and visitor studies can support informed decision-making in these areas. But too often, these studies end up being ... </p>
Superficial - they tell us what we already know</li></br>
Self-congratulatory - they tell us what we want to hear</li></br>
Abstract - they say a lot without telling us anything</li></br>
My approach is "embedded evaluation". In embedded evaluation, I team up with organizations to help them decide which questions need to be answered, how to get the asnwers, and what to do with them afterward.</p>
Interested? Find out more about what embedded evaluation is and how it can work for you.</p>


ABOUT



I chase questions about
community - outcomes - process - program design

I'm an evaluation and visitor studies consultant residing in Ann Arbor, MI. I team up with community-centered organizations, particularly libraries and museums, to help them plan, design, conduct, and use evaluation to make informed decisions. I can adapt my role to the needs of each client - I love to mentor, strategize, and plan, but I also love to get my hands dirty in methodology, data collection, and data analysis.

Graphs, venn diagrams, .cvs data files, and sticky notes (a must-have for qualitative research) are my playground.

Education
Master's degree. University of Michigan School of Information. 2008. Program focus: community information services and evaluation
Bachelor's degree. Brigham Young University. 2006. Major: English. Minor: editing.

On-the-Job

Research & Evaluation
Program Manager, IMLS National Leadership Planning Grant | Ann Arbor Hands-On Museum, Ann Arbor, MI
September 2010 - June 2011
Directed internal staff, external partners, and advisory board members in developing a state-wide model for museum-library collaboration in informal education
Led the writing of a $1M+ grant application to support prototyping of the model
Conducted a state-wide survey of librarians and informal/formal educators about Great Lakes environmental education

Evaluation & Grants Manager | Ann Arbor Hands-On Museum, Ann Arbor, MI
February 2009-July 2010
Led visitor studies and evaluation for science/children's museum, 200,000+ visitors annually
Conducted in-depth membership study, audience trends anaylsis and exhibit evaluation to increase visitation and revenue
Brought in approx. $200,000 in grant funding in less than 2 years
 
Graduate Student Research Assistant | School of Information, University of Michigan
May 2007–May 2008
Conducted research project on the effects of textual information on social behavior
Designed surveys, developed web crawler to gather online data, analyzed data for conclusions

Collection Developer | LOEX Library Instruction Clearinghouse, Eastern Michigan University
April 2007–August 2007
Conducted a front-end assessment for collection development for bibliographic instruction materials
Evaluated potential instruction resources for inclusion in collection

Program Development
Program Coordinator | Bryant Community Outreach Center, Ann Arbor, MI
October 2006–December 2006
Initiated, publicized, and successfully ran a new outreach program for parents and toddlers
Re-established relationships between the center and influential community members to increase involvement in the center
Assisted facility supervisor in grant proposals, needs assessments, staff and volunteer supervision

Recreational Staff | Boys and Girls Club, Provo, UT
April 2004–December 2004
Planned and carried out daily activities for up to 24 disadvantaged elementary students
Directed volunteer staff

Art Instructor | City of Tempe Parks and Recreation, Tempe, AZ
Summers 2002, 2003
Developed and delivered art lessons for 3 to 12 year old students based on great artists in history
Directed volunteer staff

Communications & Writing
Intern Editor | BYU Magazine, Provo, UT
January 2006–August 2006
Researched and wrote news stories for a university alumni magazine, readership 200,000
Edited feature stories by full-time editors for substance, clarity, and grammar
Conducted interviews with students, faculty, and professionals

Communications Intern | US Department of Interior, Washington, DC
May 2005–August 2005
Wrote press releases and Web site text; designed graphics and layout for press documents
Recruited mayors across the country to start local volunteer programs
Created and managed databases of programs and clients

Writing Tutor | Brigham Young University Writing Center, Provo, UT
August 2004–December 2005
Tutored students from all disciplines in writing principles
Created and revised guides and handouts in writing and research techniques



PORTFOLIO


Michigan Nature Exhibits: Dig Box & Petoskey Stone
Summative Evaluation, Ann Arbor Hands-On Museum, 2010.
Purpose: Better understand specific visitor behavior in relation to 2 exhibits in the Michigan Nature gallery.
Method: I observed 20 visitor groups to the gallery, detailing time spent in the gallery and at each exhibit, as well as behaviors and conversations. I used quantitative analysis to assess attracting power and holding time, and text coding to assess visitor behavior. 
Results: Both exhibits attracted visitors, were readily understood, and sparked social interactions. The Petoskey Stone strongly strong appealed to adults - they engaged in the exhibit without their children and verbalized more about the Stone to others. The Dig Box had a longer holding time and generate more active child use, with adult use mostly circumscribed to "parenting" behavior. 

Great Lakes Environmental Education - Libraries, 4-H and Formal Education
Opinion Survey, Hands On Our Lakes Partnership (IMLS National Leadership Planning Grant), 2010.
Purpose: Front-end evaluation to provide baseline data about science literacy and access to informal education, and better understand the needs and resources of libraries, 4-H programs, and schools relevant to informal education programs. Ultimately, help inform the development of a collaborative program for an informal education that operates on a state-wide but is based, maintained, and owned locally.
Method: I surveyed Michigan public library directors, 4-H county staff, and directors for Intermediate School Districts (130 respondents). Responses came from 57 of Michigan's 83 counties. I asked respondents a battery of questions about Great Lakes Literacy, access to and use of informal education, institutional resources, and roles they could take on in a new informal education program. The survey employed rating scales, check boxes, ranking, and open-ended response. 
Results: Librarians and 4-H staff have a shared mission for informal education but different viewpoints on how informal education plays out in their communities. More formal collaboration between librarians and 4-H could bridget this gap and increase the potential of both organizations. Libraries are compentent and comfortable in their roles as community touchstones; 4-H is ready and willing to provide local project leadership.   

What's Good For Me Is Good For Us: The effect of network structure on using social psychology to motivate contributions to a social network referral tool.
Master's thesis, University of Michigan School of Information, 2008.
Purpose: Investigate ways to motivate people within an online community to contribute to public goods. Specifically, discover whether people with different social network structures (e.g. people who are strongly connected with many others vs people who are weakly connected with just a few) react differently to motivating messages. 
Method: (Field Experiment) I emailed graduate students different messages to request their help in developing a web resource for the student body. I measured their responsiveness and compared this with data about their social network within the school.
Results: Social network structure matters when it comes to contribution behavior and reaction to motivational messages. Designers of online communities can use this to build in appropriate community structures and messaging to maximize community contributions.

Project: What's Next
Audience assessment, Ann Arbor Hands-On Museum, 2010
Purpose: Summative evaluation of a successful traveling exhibit to better understand the audience the exhibit attracted, and how to maintain that audience after the exhibit left. 
Method: I used both large-scale admissions data and structured visitor interviews to find out what effect the exhibit had on attendance, who the exhibit audience was, what they liked about the exhibit, and what the museum could do next to retain them.
Results: The exhibit caused a measurable and sizable boost in attendance. The exhibit attracted the museum's core audience and a new segment of the general public with children ages 0-4. Visitors came for the traveling exhibit because it offered both free exploration (come and go, undirected play) and variety (reason to come, see something new). Current museum exhibits and programs offer one but not both of these characteristics. The museum can use this model as a guide for developing in-house exhibits with more variety and program with more free exploration to retain their audience, as an alternative to a constant stream of expensive (often to the point of profit-less) "blockbuster" exhibits.

Hands On Science Closes the Gap for Title One Students
Summative evaluation, Ann Arbor Hands-On Museum, 2010
Purpose: Summative evaluation to explore the difference in MEAP (standardized test) scores for students who participated in the museum's Title One/Afterschool science writing program.
Method: I used the 5th grade MEAP science scores as a posttest measure. The "treatment" group was Title One students who participated in the program in the 08-09 school year. The "comparison" groups were the same year's non-Title One students, the same year's Title One students who did not participate in the program, and the previous year's Title One students (did not participate in the program).  
Results: Schools who participated in the program closed 50% of the gap between Title One and non-Title One students on the 5th grade science MEAP. These grade-level gains show are for students at all levels of performance, not just those who were bordering on the pass line.


#####################################################

Action For Earth Evaluation
Ann Arbor Hands-On Museum, 2010
<p>Type: Outcome evaluation</p>
<p>Purpose: Understand impacts, challenges, and motivations of pilot program Action for Earth (conducted by museum in collaboration with librarians) to inform future derivations of the program.</p>
<p>Method: Surveys of participating families (parents) and librarians, observations of program activities</p>

Library-Based Deliberative Democracy Forums
<p>University of Michigan School of Information, 2007</p>
<p>Type: Outcome evaluation</p>
<p>Purpose: Evaluate candidate outcomes of deliberative democracy training and forums for librarians, libraries and communities.</p>
<p>Method: Structured, in depth interviews of librarians</p>

Michigan Nature Exhibits: Dig Box & Petoskey Stone
<p>Ann Arbor Hands-On Museum, 2010</p>
<p>Type: Summative Evaluation</p>
<p>Purpose: Better understand specific visitor behavior at 2 exhibits in the Michigan Nature gallery.</p>
<p>Method: Observations of visitor groups</p>

Great Lakes Environmental Education - Libraries, 4-H and Formal Education
<p>Hands On Our Lakes Partnership (IMLS National Leadership Planning Grant), 2010</p>
<p>Type: Front-end evaluation</p>
<p>Purpose: Provide baseline data about science literacy and access to informal education, and better understand the needs and resources of libraries, 4-H programs, and schools relevant to informal education programs. Ultimately, help inform the development of a collaborative program for an informal education that operates on a state-wide but is based, maintained, and owned locally.</p>
<p>Method: Opinion survey of Michigan public library directors, 4-H county staff, and directors for Intermediate School Districts</p>

What's Good For Me Is Good For Us: The effect of network structure on using social psychology to motivate contributions to a social network referral tool.
<p>University of Michigan School of Information, 2008</p>
<p>Type: Master's thesis</p>
<p>Purpose: Investigate ways to motivate people within an online community to contribute to public goods. Specifically, discover whether people with different social network structures (e.g. people who are strongly connected with many others vs people who are weakly connected with just a few) react differently to motivating messages. </p>
<p>Method: Social network analysis, field experiment with 2 treatments and control</p>

Project: What's Next
<p>Ann Arbor Hands-On Museum, 2010</p>
<p>Type: Audience assessment/Summative evaluation</p>
<p>Purpose: Find out what audience segment was attracted by a successful traveling exhibit and how to maintain that audience after the exhibit left. </p>
<p>Method: Large-scale admissions data, structured visitor interviews</p>

Hands On Science Closes the Gap for Title One Students
<p>Ann Arbor Hands-On Museum, 2010</p>
<p>Type: Summative evaluation</p>
<p>Purpose: Explore the difference in MEAP (standardized test) scores for students who participated in the museum's Title One/Afterschool science writing program.</p>
<p>Method: Posttest measure (5th grade MEAP science scores) with 1 treatment and several comparison groups.</p>

Membership Study
<p>Ann Arbor Hands-On Museum, 2009</p>
<p>Type: Audience assessment</p>
<p>Purpose: Discover who museum members are, why they become members and then renew or do not renew their memberships.</p>
<p>Method: Semi-structured interviews and survey of members</p>


APPROACH

Overview:
Integrated evaluation is an approach to visitor studies and program evaluation that plans for how the evaluation will be actively used by an organization to make decisions or demonstrate impact. Instead of starting with the question "What do we want to know?", we start with the question "Why do we want to know?" After we decide why we're doing an evaluation in the first place, we then hone in on essential questions, develop an evaluation plan, execute that plan, and then follow through with specific action steps.

I promise to do 3 things as an evaluator:
1. Plan for useability: I work with you to develop a plan for how to effectively use your evaluation to make decisions, inform strategies and get stakeholder attention.
2. Construct memorable frameworks: I translate evaluation findings into fundamental frameworks that become a common language for your team to use for integrating those findings in day-to-day decision-making.
3. Work efficiently: I get more done in less time and with fewer resources.

Integrated evaluation and grant writing
We have questions about our audience, programs or services. We find answers through evaluation. We make decisions, set strategic priorities, and know what we want to do next. Often, the next step is to gather the resources to get there. 

Integrated evaluations can translate almost directly into competitive grant applications. Grant makers ask you to tell them about your goals, audience needs, your program/service model, demonstrated success, opportunities and plans for growth, measures for future success. These questions are answered in the course of conducting an integrated evaluation - the evaluation gives you the concepts, the data, and even the written language to build competitive funding requests.

Planning the end in the beginning - questions to ask yourself before you start asking questions of your visitors:
What is the purpose of this evaluation? Why is this important?
What decisions will the evaluation help you make?
What decisions will be made in the 3 months following the evaluation?
Who will be part of making these decisions? Who will see the results of the evaluation?
Who needs to be part of the evaluation team?


